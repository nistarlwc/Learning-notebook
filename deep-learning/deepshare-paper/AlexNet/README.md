# ALexNet
## 网络
![网络结构](439761-20190129114344192-623663293.jpg)


![卷积计算](20190630163604600.PNG)  
注意：全连接中“+1”指的是bias

## 激活函数
![激活函数](2020-09-02-10-39-51的屏幕截图.png) 
### ReLU Nonlinearity  
优点：  
1.使网络训练更快  
2.防止梯度消失(gradient vanishing)  
3.使网络具有稀疏性   
4.不需要输入进行标准化来防止饱和现象，sigmod/tanh则必须要标准化

ReLU也有几个需要特别注意的问题：
1.ReLU的输出不是zero-centered(零均值化)
2.Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: 
(1) 非常不幸的参数初始化，这种情况比较少见 
(2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用**Xavier初始化**方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

### Local Response Normalization 局部相应归一化
为什么要引入LRN层？  
首先要引入一个神经生物学的概念：侧抑制（lateral inhibitio），即指被激活的神经元抑制相邻的神经元。归一化（normaliazation）的目的就是“抑制”,LRN就是借鉴这种侧抑制来实现局部抑制，尤其是我们使用RELU的时候，这种“侧抑制”很有效 ，因而在alexnet里使用有较好的效果。

归一化有什么好处？  
1.归一化有助于快速收敛；  
2.对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

## droput
droput在inference时，需要对输出乘以droput系数